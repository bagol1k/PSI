#' # ANALIZA TEXT MINING


#' #### ğŸ“Œ Przetwarzanie i oczyszczanie tekstu <br>*(Text Preprocessing and Text Cleaning)*
#'
#' - wczytanie tekstu z odpowiednim kodowaniem (UTF-8)
#' - normalizacja (ujednolicenie) wielkoÅ›ci liter (zamiana na maÅ‚e litery = lowercase)
#' - normalizacja (ujednolicenie) rozbieÅ¼nych kodowaÅ„ znakÃ³w (apostrofy, cudzysÅ‚owy)
#' - normalizacja (ujednolicenie) form skrÃ³conych (I'm, I've, don't) przez usuniÄ™cie lub rozwiniÄ™cie
#' - normalizacja (ujednolicenie) rÃ³Å¼nych akcentÃ³w ("cafÃ©" na "cafe") przez usuniÄ™cie akcentÃ³w
#' - normalizacja (ujednolicenie) popularnych skrÃ³tÃ³w ("btw" na "by the way", "b4" na "before") przez rozwiniÄ™cie
#' - usuniÄ™cie zbÄ™dnych ciÄ…gÃ³w znakÃ³w (adresy URL, tagi HTML)
#' - usuniÄ™cie zbÄ™dnych znakÃ³w specjalnych (*, &, #, @, $)
#' - usuniÄ™cie zbÄ™dnych biaÅ‚ych znakÃ³w (spacja, tabulacja, znak przejÅ›cia do nowej linii "enter")
#' - usuniÄ™cie wybranych sÅ‚Ã³w nie wnoszÄ…cych znaczenia dla tekstu (stopwords)
#' - usuniÄ™cie cyfr i liczb
#' - usuniÄ™cie interpunkcji
#' - tokenizacja (podziaÅ‚ tekstu na sÅ‚owa = tokeny)
#' - usuniÄ™cie stopwords (sÅ‚Ã³w o maÅ‚ej wartoÅ›ci semantycznej, np. "the", "and")
#' - usuniÄ™cie pustych elementÃ³w (rozwaÅ¼enie problemu brakujÄ…cych/niekompletnych danych )
#' - stemming (sprowadzenie sÅ‚Ã³w do ich rdzenia/formy podstawowej)
#'


#' #### ğŸ“Œ Zliczanie czÄ™stoÅ›ci sÅ‚Ã³w i prezentacja danych <br>
#' - zliczanie czÄ™stoÅ›ci sÅ‚Ã³w i prezentacja wynikÃ³w np. za pomoca chmury sÅ‚Ã³w


#' #### ğŸ“Œ Analiza sentymentu i prezentacja danych <br>
#' - analiza i dopasowanie sentymentu do konkrennych sÅ‚Ã³ dla sÅ‚ownikÃ³w Loughran i NRC
#' - wykresy sÅ‚upkowe pokazujÄ…ce rozkÅ‚ad sentymentu
#' - analiza sentymentu dla kawaÅ‚kÃ³w tekstu tzn. np. podziaÅ‚ na negatywny i pozytywny sentyment za pomocÄ… sÅ‚ownikÃ³w GI, HE, LM
#' - wykres sÅ‚upkowy przedstawiajÄ…cy podziaÅ‚ sentymentu w caÅ‚ym tekÅ›cie dla trzech wspomnianych sÅ‚ownikÃ³w na raz
#' - analiza zmieniajÄ…cego siÄ™ sentymentu w tekÅ›cie w zaleÅ¼noÅ›ci od momentu tekstu dla sÅ‚ownikÃ³w GI, HE, LM
#' - prezentacja zmieniajÄ…cego siÄ™ sentymentu na wykresie
#' - doÅ‚Ä…czenie zdjÄ™cia zachowania rynku podaczas wystÄ…pienia

#' #### ğŸ“Œ Topic modeling i prezentacja danych <br>
#' - podziaÅ‚ sÅ‚Ã³w rÃ³Å¼ne kategorie tematyczne wg algorytmu LDA
#' - przedstawienie obszarÃ³w tematycznych na wykresach z podziaÅ‚em na odpowiednio 2, 3 i 4 tematy


library(tm)
library(tidytext)
library(stringr)
library(wordcloud)
library(RColorBrewer)
library(ggplot2)
library(SnowballC)
library(SentimentAnalysis)
library(ggthemes)
library(tidyverse)
library(topicmodels)
library(dplyr)


#WybÃ³r dokumentu do przetwarzania
#wektor z Å›cieÅ¼kami dokumentÃ³w
docs <- c(
  "fomcpresconf20220615.txt",
  "fomcpresconf20240918.txt",
  "fomcpresconf20250319.txt",
  "fomcpresconf20250507.txt"
)

pics <- c(
  "220615D1.png",
  "240918H1.png",
  "250319M15.png",
  "250507M5.png"
)

# pytanie uzytkownika o wybor
if (interactive()) { #zeby w kompilowaniu raportu dzialalo
  cat("Wybierz dokument 1â€“4:\n"); for(i in seq_along(docs)) cat(i,":",docs[i],"\n")
  choice <- as.integer(readline("Podaj numer: "))
} else {
  message("Tryb nieinteraktywny â€“ wybieram domyÅ›lnie dokument 1")
  choice <- 1
}

#sprawdzanie czy wybor jest poprawny
if (is.na(choice) || choice<1 || choice>length(docs)) {
  stop("Niepoprawny numer â€“ musi byÄ‡ od 1 do 4.")
}

# wczytanie wybranego pliku
file_path <- docs[choice]
zdjecie <- pics[choice]


process_text <- function(file_path) {
  text <- tolower(readLines(file_path, encoding = "UTF-8"))
  text <- gsub("[\u2019\u2018\u0060\u00B4]", "'", text) #jednolite apostrofy
  text <- removeNumbers(text)
  words <- unlist(strsplit(text, "\\s+")) #splitowanie na spacjach
  words <- words[words != ""]
  words <- words[!str_detect(words, "'")] #usuwa slowa z apostrofami
  words <- str_replace_all(words, "[[:punct:]]", "") #usuwa znaki interpunkcyjne
  words <- words[words != ""]
  words <- str_trim(words) #usuwa znaki biale 'przy' sÅ‚owach
  
  tidy_stopwords <- tolower(stop_words$word)
  tidy_stopwords <- gsub("[\u2019\u2018\u0060\u00B4]", "'", tidy_stopwords)
  tm_stopwords <- tolower(stopwords("en"))
  tm_stopwords <- gsub("[\u2019\u2018\u0060\u00B4]", "'", tm_stopwords)

  words <- words[!(words %in% tidy_stopwords)]
  words <- words[!(words %in% tm_stopwords)]
  
  stemmed_doc <- stemDocument(words)
  completed_doc <- stemCompletion(stemmed_doc, dictionary=words, type="prevalent")
  completed_doc <- completed_doc[completed_doc != ""]
  
  completed_doc
}

word_frequency <- function(words) {
  freq <- table(words)
  freq_df <- data.frame(word = names(freq), freq = unclass(freq))
  freq_df <- freq_df[order(-freq_df$freq), ] #malejÄ…co
  return(freq_df)
}


plot_wordcloud <- function(freq_df, title, color_palette = "Dark2") {
  wordcloud(words = freq_df$word, freq = freq_df$freq, min.freq = 14,
            colors = brewer.pal(8, color_palette), random.order = FALSE) #zeby najwieksze slowo bylo na srodku
  title(title, cex.main = 2)
}

words <- process_text(file_path)

custom_stopwords <- c("$", "chair", "powell", "page", "november", "march", "june", "september", "press", "conference", "question", "michelle", "smith")
words <- words[!words %in% custom_stopwords]

freq_df <- word_frequency(words)
plot_wordcloud(freq_df, "Chmura sÅ‚Ã³w")

loughran <- read.csv("loughran.csv", stringsAsFactors = FALSE)
nrc <- read.csv("nrc.csv", stringsAsFactors = FALSE)


tidy_tokeny <- as_tibble(freq_df) #Å‚adniejsze freq_df


#' # Analiza sentymentu przy uÅ¼yciu sÅ‚ownika Loughran
#zostanÄ… tylko sÅ‚owa ktore wystepuja w slowniku

# Zliczanie sentymentu
sentiment_review <- tidy_tokeny %>%
  inner_join(loughran, relationship = "many-to-many")

# Zliczanie, ktÃ³re sÅ‚owa sÄ… najczÄ™stsze
# dla danego sentymentu
sentiment_review %>%
  group_by(sentiment) %>%
  arrange(desc(freq)) %>%
  ungroup()

# Filtrowanie analizy sentymentu

sentiment_review2 <- sentiment_review %>%
  filter(sentiment %in% c("positive", "negative", "uncertainty")) #wybieramy niektÃ³re sentymenty, poza tym innych jest malo


word_counts <- sentiment_review2 %>%
  group_by(sentiment) %>%
  top_n(20, freq) %>%
  ungroup() %>%
  arrange(desc(freq), word) %>%
  mutate(
    word2 = factor(word, levels = rev(unique(word))) #dodajemy te kolumne, zeby potem na wykresie rysowac zgodnie z rosnaca kolejnoscia
  )

# Wizualizacja sentymentu
ggplot(word_counts[1:30,], aes(x=word2, y=freq, fill=sentiment)) + 
  geom_col(show.legend=FALSE) +
  facet_wrap(~sentiment, scales="free") +
  coord_flip() +
  labs(x = "SÅ‚owa", y = "Liczba wystÄ…pieÅ„") +
  theme_gdocs() + 
  ggtitle("Liczba sÅ‚Ã³w wg sentymentu (Loughran)") +
  scale_fill_manual(values = c("firebrick", "darkolivegreen4","lightblue"))


#' # Analiza sentymentu przy uÅ¼yciu sÅ‚ownika NRC

# Zliczanie sentymentu
sentiment_review_nrc <- tidy_tokeny %>%
  inner_join(nrc, relationship = "many-to-many")

# Zliczanie, ktÃ³re sÅ‚owa sÄ… najczÄ™stsze
# dla danego sentymentu
sentiment_review_nrc %>%
  group_by(sentiment) %>%
  arrange(desc(freq)) %>%
  ungroup()

# Filtrowanie analizy sentymentu

sentiment_review_nrc2 <- sentiment_review_nrc %>%
  filter(sentiment %in% c("positive", "negative", "anticipation", "fear"))

word_counts_nrc2 <- sentiment_review_nrc2 %>%
  group_by(sentiment) %>%
  top_n(20, freq) %>%
  ungroup() %>%
  arrange(desc(freq), word) %>%
  mutate(
    word2 = factor(word, levels = rev(unique(word)))
  )

# Wizualizacja sentymentu
ggplot(word_counts_nrc2[1:30,], aes(x=word2, y=freq, fill=sentiment)) + 
  geom_col(show.legend=FALSE) +
  facet_wrap(~sentiment, scales="free") +
  coord_flip() +
  labs(x = "SÅ‚owa", y = "Liczba wystÄ…pieÅ„") +
  theme_gdocs() + 
  ggtitle("Liczba sÅ‚Ã³w wg sentymentu (NRC)")


#' # Analiza sentymentu w czasie o ustalonej dÅ‚ugoÅ›ci linii

# PoÅ‚Ä…czenie wszystkich sÅ‚Ã³w words w jeden ciÄ…g znakÃ³w oddzielony spacjami
full_text <- paste(words, collapse = " ")


# Funkcja do dzielenia tekstu na segmenty o okreÅ›lonej dÅ‚ugoÅ›ci
split_text_into_chunks <- function(text, chunk_size) {
  start_positions <- seq(1, nchar(text), by = chunk_size)
  chunks <- substring(text, start_positions, start_positions + chunk_size - 1)
  
  chunks
}

# Podzielenie tekstu na segmenty
text_chunks <- split_text_into_chunks(full_text, 30)

#' # Analiza sentymentu przy uÅ¼yciu pakietu SentimentAnalysis
sentiment <- analyzeSentiment(text_chunks)


#' # SÅ‚ownik GI (General Inquirer)
# SÅ‚ownik ogÃ³lnego zastosowania
# zawiera listÄ™ sÅ‚Ã³w pozytywnych i negatywnych
# zgodnych z psychologicznym sÅ‚ownikiem harwardzkim Harvard IV-4


# Konwersja ciÄ…gÅ‚ych wartoÅ›ci sentymentu 
# na odpowiadajÄ…ce im wartoÅ›ci kierunkowe 
# zgodnie ze sÅ‚ownikiem GI
sentimentGI <- convertToDirection(sentiment$SentimentGI) #zamienia liczby reprezentujace sentyment na np. positive / negative

# Konwersja do ramki danych (ggplot wizualizuje ramki danych)
df_GI <- data.frame(index = seq_along(sentimentGI), value = sentimentGI, Dictionary = "GI")
df_GI <- na.omit(df_GI)



#' # SÅ‚ownik HE (Henryâ€™s Financial dictionary)
#
# zawiera listÄ™ sÅ‚Ã³w pozytywnych i negatywnych
# zgodnych z finansowym sÅ‚ownikiem "Henry 2008"
# pierwszy, jaki powstaÅ‚ w wyniku analizy komunikatÃ³w prasowych 
# dotyczÄ…cych zyskÃ³w w branÅ¼y telekomunikacyjnej i usÅ‚ug IT


# Konwersja ciÄ…gÅ‚ych wartoÅ›ci sentymentu 
# na odpowiadajÄ…ce im wartoÅ›ci kierunkowe 
# zgodnie ze sÅ‚ownikiem HE
sentimentHE <- convertToDirection(sentiment$SentimentHE)


df_HE <- data.frame(index = seq_along(sentimentHE), value = sentimentHE, Dictionary = "HE")
df_HE <- na.omit(df_HE)



#' # SÅ‚ownik LM (Loughran-McDonald Financial dictionary)
#
# zawiera listÄ™ sÅ‚Ã³w pozytywnych i negatywnych oraz zwiÄ…zanych z niepewnoÅ›ciÄ…
# zgodnych z finansowym sÅ‚ownikiem Loughran-McDonald
# DictionaryLM


# Konwersja ciÄ…gÅ‚ych wartoÅ›ci sentymentu 
# na odpowiadajÄ…ce im wartoÅ›ci kierunkowe 
# zgodnie ze sÅ‚ownikiem LM
sentimentLM <- convertToDirection(sentiment$SentimentLM)

df_LM <- data.frame(index = seq_along(sentimentLM), value = sentimentLM, Dictionary = "LM")
df_LM <- na.omit(df_LM)


# PoÅ‚Ä…czenie poszczegÃ³lnych ramek w jednÄ… ramkÄ™
df_all <- bind_rows(df_GI, df_HE, df_LM)

# Tworzenie wykresu z podziaÅ‚em na sÅ‚owniki
ggplot(df_all, aes(x = value, fill = Dictionary)) +
  geom_bar(alpha = 0.4) + 
  labs(title = "Skumulowany sentyment wedÅ‚ug sÅ‚ownikÃ³w", x = "Sentyment", y = "Liczba") +
  theme_bw() +
  facet_wrap(~Dictionary) +  # PodziaÅ‚ na cztery osobne wykresy
  scale_fill_manual(values = c("GI" = "green", "HE" = "blue", "LM" = "orange"))



#' # Agregowanie sentymentu z rÃ³Å¼nych sÅ‚ownikÃ³w w czasie
# Utworzenie ramki danych
df_all <- data.frame(sentence=1:length(sentiment[,1]),
                     GI=sentiment$SentimentGI, 
                     HE=sentiment$SentimentHE, 
                     LM=sentiment$SentimentLM)


# USUNIÄ˜CIE BRAKUJÄ„CYCH WARTOÅšCI
# gdyÅ¼ wartoÅ›ci NA (puste) uniemoÅ¼liwiajÄ… generowanie wykresu w ggplot
df_all <- df_all[complete.cases(df_all), ]

#' # Wykresy przedstawiajÄ…ce ewolucjÄ™ sentymentu w czasie
ggplot(df_all, aes(x=sentence, y=LM)) + 
  geom_smooth(color="red") +
  geom_smooth(aes(x=sentence, y=GI), color="green") +
  geom_smooth(aes(x=sentence, y=HE), color="blue") +
  labs(x = "OÅ› czasu zdaÅ„", y = "Sentyment") +
  theme_gdocs() + 
  ggtitle("Zmiana sentymentu w czasie")

knitr::include_graphics(zdjecie)



# Funkcja do modelowania tematÃ³w LDA
top_terms_by_topic_LDA <- function(input_text, plot = TRUE, k = number_of_topics) {    
  # UtwÃ³rz korpus z oczyszczonego tekstu
  corpus <- VCorpus(VectorSource(input_text)) #1 ciag tekstu = 1 dokument
  DTM <- DocumentTermMatrix(corpus)
  
  # UsuÅ„ puste wiersze w macierzy DTM
  unique_indexes <- unique(DTM$i) #kolumna 'i' zawiera indeksy wierszy w ktorych sa jakies dane
  DTM <- DTM[unique_indexes,] #czyli wybieramy te wierzse w ktorych cos jest
  
  # SprawdÅº, czy DTM nie jest pusty
  if (nrow(DTM) == 0) {
    stop("Macierz DTM jest pusta. SprawdÅº dane wejÅ›ciowe.")
  }
  
  # Wykonaj LDA
  lda <- LDA(DTM, k = number_of_topics, control = list(seed = 1234))
  topics <- tidy(lda, matrix = "beta")
  
  # Pobierz 10 najczÄ™stszych sÅ‚Ã³w dla kaÅ¼dego tematu
  top_terms <- topics %>%
    group_by(topic) %>%
    top_n(10, beta) %>%
    ungroup() %>%
    arrange(topic, -beta)
  
  # Rysuj wykres, jeÅ›li plot = TRUE
  if(plot) {
    top_terms %>%
      mutate(term = reorder(term, beta)) %>% #sortuje terminy wg waÅ¼noÅ›ci
      ggplot(aes(term, beta, fill = factor(topic))) +
      geom_col(show.legend = FALSE) +
      facet_wrap(~ topic, scales = "free") +
      labs(x = "Terminy", y = "Î² (waÅ¼noÅ›Ä‡ sÅ‚owa w temacie)") +
      coord_flip() +
      theme_minimal() +
      scale_fill_brewer(palette = "Set1")
  } else {
    top_terms
  }
}

# Modelowanie tematÃ³w LDA dla rÃ³Å¼nej liczby tematÃ³w
number_of_topics <- 2
cat("\nLDA dla", number_of_topics, "tematÃ³w:\n")
top_terms_by_topic_LDA(full_text)

number_of_topics <- 3
cat("\nLDA dla", number_of_topics, "tematÃ³w:\n")
top_terms_by_topic_LDA(full_text)

number_of_topics <- 4
cat("\nLDA dla", number_of_topics, "tematÃ³w:\n")
top_terms_by_topic_LDA(full_text)
